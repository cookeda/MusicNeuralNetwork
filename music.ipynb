{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.backend import clear_session\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.losses import mean_squared_error, kullback_leibler_divergence\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(endpoint='loudness', subset='train'):\n",
    "    \"\"\"\n",
    "    Gets the data matrix, X, and target, y for the desired 'endpoint'.\n",
    "\n",
    "    :param endpoint: 'loudness', 'pitch', 'timbre', 'loudness', 'chroma', 'mfcc', 'spectral\n",
    "    :param subset: 'train', 'valid'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    npz_file = f'music_{endpoint}.npz'\n",
    "    with np.load(npz_file) as data:\n",
    "        x = data[f'x_{subset}']\n",
    "        y = data[f'y_{subset}']\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape((-1, 1))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def setup_model_checkpoints(output_path):\n",
    "    \"\"\"\n",
    "    Setup model checkpoints using the save path and frequency.\n",
    "\n",
    "    :param output_path: The directory to store the checkpoints in\n",
    "    :return: a ModelCheckpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(output_path, 'model.{epoch:05d}_{val_loss:f}.h5'),\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch',\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "    return model_checkpoint\n",
    "\n",
    "\n",
    "def visualize(model, endpoint='loudness', subset='valid', output_path=''):\n",
    "    \"\"\"\n",
    "    Create a joint distribution plot that shows relationship between\n",
    "    model estimates and true values.\n",
    "\n",
    "    :param model: A trained model\n",
    "    :param endpoint: The name of the target\n",
    "    :param subset: Which data set to use\n",
    "    :param output_path: The output directory to save the PNG\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    bins = 128\n",
    "\n",
    "    x, y_true = get_xy(endpoint, subset)\n",
    "\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    # check assumptions\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(f'WARNING: output should have shape {y_true.shape} not {y_pred.shape}. Broadcasting output.')\n",
    "        y_pred = np.broadcast_to(y_pred, y_true.shape)\n",
    "\n",
    "    # loss should be mean squared error unless pitch target\n",
    "    metric = mean_squared_error\n",
    "\n",
    "    if endpoint in ('pitch', 'chroma'):\n",
    "        # if each row of target sums to one and is nonnegative, we know it must be pitches.\n",
    "        metric = kullback_leibler_divergence\n",
    "        if not (y_pred >= 0).all():\n",
    "            print(f'WARNING: output should be nonnegative. Setting negative values to a small positive value.')\n",
    "            y_pred[y_pred < 0] = 1e-6\n",
    "        if not np.allclose(a=y_pred.sum(axis=1), b=1):\n",
    "            print(f'WARNING: outputs should sum to one. Scaling rows of output to sum to one.')\n",
    "            y_pred = y_pred / y_pred.sum(axis=1, keepdims=True)\n",
    "\n",
    "    loss = tf.reduce_mean(metric(y_true, y_pred)).numpy()\n",
    "\n",
    "    # make joint plot\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        num_targets = y_true.shape[1]\n",
    "        num_cols = int(np.ceil(num_targets ** (1/2)))\n",
    "        num_rows = int(np.ceil(num_targets / num_cols))\n",
    "        png_file = os.path.join(output_path, f'visualize_{subset}.png')\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*3, num_rows*3),\n",
    "                                 gridspec_kw=dict(hspace=0.0, wspace=0.0))\n",
    "        if isinstance(axes, plt.Axes):\n",
    "            axes = np.array([axes])\n",
    "        for j, axi in enumerate(axes.flat[:num_targets]):\n",
    "            # make joint plot\n",
    "            jg = sns.jointplot(x=y_true[:, j], y=y_pred[:, j], kind='hist',\n",
    "                               joint_kws=dict(bins=bins), marginal_kws=dict(bins=bins))\n",
    "            if metric == mean_squared_error:\n",
    "                # for MSE we can assign loss to each target\n",
    "                loss_j = ((y_pred[:, j] - y_true[:, j])**2).mean()\n",
    "                jg.fig.suptitle(f'{endpoint}[{j}] loss = {loss_j:8g}')\n",
    "            else:\n",
    "                # kullback-leibler divergence cannot be divided among targets\n",
    "                jg.fig.suptitle(f'{endpoint}[{j}]')\n",
    "            jg.set_axis_labels(xlabel='Actual', ylabel='Model')\n",
    "            xlm = plt.xlim()\n",
    "            ylm = plt.ylim()\n",
    "            max_value = max(max(xlm), max(ylm))\n",
    "            min_value = min(min(xlm), min(ylm))\n",
    "            jg.ax_joint.plot([min_value, max_value], [min_value, max_value], color='k', linestyle='--')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # save joint plot in image\n",
    "            jg.fig.canvas.draw()\n",
    "            image_flat = np.frombuffer(jg.fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "            image = image_flat.reshape(*reversed(jg.fig.canvas.get_width_height()), 3)  #\n",
    "            plt.close(jg.fig)\n",
    "\n",
    "            # show image in subplots figure\n",
    "            axi.imshow(image)\n",
    "            axi.set(xticks=[], yticks=[])\n",
    "        fig.suptitle(f'{endpoint} {subset} loss = {loss:8g}')\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(fname=png_file, dpi=300)\n",
    "\n",
    "\n",
    "def get_best_model(output_path):\n",
    "    \"\"\"\n",
    "    Parses the output_path to find the best model. Relies on the ModelCheckpoint\n",
    "    saving a file name with the validation loss in it. If a model was saved with\n",
    "    a Normalization layer, it's provided as a custom object.\n",
    "\n",
    "    :param output_path: The directory to scan for H5 files\n",
    "    :return: The best model compiled.\n",
    "    \"\"\"\n",
    "    min_loss = float('inf')\n",
    "    best_model_file = None\n",
    "    best_epoch = None\n",
    "    for file_name in os.listdir(output_path):\n",
    "        if file_name.endswith('.h5'):\n",
    "            try:\n",
    "                val_loss = float('.'.join(file_name.split('_')[1].split('.')[:-1]))\n",
    "                epoch = int(file_name.split('.')[1].split('_')[0])\n",
    "                if val_loss < min_loss:\n",
    "                    best_model_file = file_name\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = epoch\n",
    "            except IndexError:\n",
    "                pass\n",
    "    print(f'loading best model: {best_model_file}')\n",
    "    model = load_model(os.path.join(\n",
    "        output_path, best_model_file), compile=True)\n",
    "    return model, best_epoch, min_loss\n",
    "\n",
    "\n",
    "def plot_history(history, output_path='.'):\n",
    "    keys = [k for k in history.history.keys() if not k.startswith('val_')]\n",
    "\n",
    "    num_cols = int(np.ceil(len(keys) ** (1/2)))\n",
    "    num_rows = int(np.ceil(len(keys) / num_cols))\n",
    "    plt.figure()\n",
    "    for i, k in enumerate(keys):\n",
    "        ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        ax.plot(history.history[k], label=k)\n",
    "        val_key = f'val_{k}'\n",
    "        if val_key in history.history:\n",
    "            ax.plot(history.history[val_key], label=val_key)\n",
    "        plt.legend()\n",
    "    plt.savefig(os.path.join(output_path, 'learning_curve.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "x_train, y_train = get_xy(endpoint='timbre', subset='train')\n",
    "x_valid, y_valid = get_xy(endpoint='timbre', subset='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization, Normalization, LeakyReLU, ELU, ReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "normalize = Normalization(axis=-1)\n",
    "normalize.adapt(x_train)\n",
    "\n",
    "# Step 2: Model Architecture\n",
    "clear_session()\n",
    "model = Sequential([\n",
    "    Input(shape=(x_train.shape[1],)),  # Input layer\n",
    "    normalize,\n",
    "    Dense(256, activation='tanh', kernel_regularizer=l2(0.01)),\n",
    "    Dense(128, activation='swish'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='elu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(64, activation='elu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),  \n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compile the Model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Callbacks\n",
    "output_path = 'music_timbre_regression' \n",
    "model_checkpoint = setup_model_checkpoints(output_path)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 1026.6647 - mse: 1022.4625\n",
      "Epoch 1: val_loss improved from inf to 810.38135, saving model to music_timbre_regression\\model.00001_810.381348.h5\n",
      "85/85 [==============================] - 5s 37ms/step - loss: 1026.6647 - mse: 1022.4625 - val_loss: 810.3813 - val_mse: 805.8127\n",
      "Epoch 2/500\n",
      " 1/85 [..............................] - ETA: 5s - loss: 790.7127 - mse: 786.1440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/85 [============================>.] - ETA: 0s - loss: 723.4228 - mse: 718.7956\n",
      "Epoch 2: val_loss improved from 810.38135 to 626.37268, saving model to music_timbre_regression\\model.00002_626.372681.h5\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 722.6898 - mse: 718.0631 - val_loss: 626.3727 - val_mse: 621.8020\n",
      "Epoch 3/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 559.9455 - mse: 555.4333\n",
      "Epoch 3: val_loss improved from 626.37268 to 478.65878, saving model to music_timbre_regression\\model.00003_478.658783.h5\n",
      "85/85 [==============================] - 3s 39ms/step - loss: 559.5186 - mse: 555.0070 - val_loss: 478.6588 - val_mse: 474.2024\n",
      "Epoch 4/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 488.4185 - mse: 483.9600\n",
      "Epoch 4: val_loss improved from 478.65878 to 444.15359, saving model to music_timbre_regression\\model.00004_444.153595.h5\n",
      "85/85 [==============================] - 6s 66ms/step - loss: 488.1460 - mse: 483.6873 - val_loss: 444.1536 - val_mse: 439.6809\n",
      "Epoch 5/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 460.8676 - mse: 456.3856\n",
      "Epoch 5: val_loss improved from 444.15359 to 422.80359, saving model to music_timbre_regression\\model.00005_422.803589.h5\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 460.8676 - mse: 456.3856 - val_loss: 422.8036 - val_mse: 418.3195\n",
      "Epoch 6/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 446.4755 - mse: 441.9926\n",
      "Epoch 6: val_loss improved from 422.80359 to 414.87213, saving model to music_timbre_regression\\model.00006_414.872131.h5\n",
      "85/85 [==============================] - 5s 58ms/step - loss: 446.2963 - mse: 441.8135 - val_loss: 414.8721 - val_mse: 410.3935\n",
      "Epoch 7/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 435.9149 - mse: 431.4333\n",
      "Epoch 7: val_loss improved from 414.87213 to 414.53723, saving model to music_timbre_regression\\model.00007_414.537231.h5\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 435.7661 - mse: 431.2845 - val_loss: 414.5372 - val_mse: 410.0494\n",
      "Epoch 8/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 428.4448 - mse: 423.9533\n",
      "Epoch 8: val_loss improved from 414.53723 to 414.42508, saving model to music_timbre_regression\\model.00008_414.425079.h5\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 428.3492 - mse: 423.8577 - val_loss: 414.4251 - val_mse: 409.9303\n",
      "Epoch 9/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 423.3943 - mse: 418.9009\n",
      "Epoch 9: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 423.3943 - mse: 418.9009 - val_loss: 416.2785 - val_mse: 411.7845\n",
      "Epoch 10/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 415.7504 - mse: 411.2519\n",
      "Epoch 10: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 415.7504 - mse: 411.2519 - val_loss: 422.9861 - val_mse: 418.4855\n",
      "Epoch 11/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 415.3971 - mse: 410.9039\n",
      "Epoch 11: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 415.2495 - mse: 410.7563 - val_loss: 418.0882 - val_mse: 413.5928\n",
      "Epoch 12/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 411.4659 - mse: 406.9729\n",
      "Epoch 12: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 411.4659 - mse: 406.9729 - val_loss: 428.0398 - val_mse: 423.5466\n",
      "Epoch 13/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 405.9815 - mse: 401.4837\n",
      "Epoch 13: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 405.9085 - mse: 401.4107 - val_loss: 425.5895 - val_mse: 421.0938\n",
      "Epoch 14/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 404.4780 - mse: 399.9849\n",
      "Epoch 14: val_loss did not improve from 414.42508\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 404.4780 - mse: 399.9849 - val_loss: 418.5587 - val_mse: 414.0657\n",
      "Epoch 15/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 397.6917 - mse: 393.1860\n",
      "Epoch 15: val_loss improved from 414.42508 to 412.66360, saving model to music_timbre_regression\\model.00015_412.663605.h5\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 397.6917 - mse: 393.1860 - val_loss: 412.6636 - val_mse: 408.1433\n",
      "Epoch 16/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 388.0283 - mse: 383.5005\n",
      "Epoch 16: val_loss improved from 412.66360 to 405.06686, saving model to music_timbre_regression\\model.00016_405.066864.h5\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 388.0283 - mse: 383.5005 - val_loss: 405.0669 - val_mse: 400.5303\n",
      "Epoch 17/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 382.1666 - mse: 377.6182\n",
      "Epoch 17: val_loss did not improve from 405.06686\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 382.0736 - mse: 377.5251 - val_loss: 408.3668 - val_mse: 403.8087\n",
      "Epoch 18/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 378.8574 - mse: 374.2921\n",
      "Epoch 18: val_loss improved from 405.06686 to 402.07867, saving model to music_timbre_regression\\model.00018_402.078674.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 378.8574 - mse: 374.2921 - val_loss: 402.0787 - val_mse: 397.5082\n",
      "Epoch 19/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 377.2540 - mse: 372.6773\n",
      "Epoch 19: val_loss did not improve from 402.07867\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 377.0851 - mse: 372.5084 - val_loss: 406.7894 - val_mse: 402.2114\n",
      "Epoch 20/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 372.1065 - mse: 367.5197\n",
      "Epoch 20: val_loss did not improve from 402.07867\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 372.1065 - mse: 367.5197 - val_loss: 412.9578 - val_mse: 408.3611\n",
      "Epoch 21/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 370.0003 - mse: 365.4006\n",
      "Epoch 21: val_loss improved from 402.07867 to 401.14334, saving model to music_timbre_regression\\model.00021_401.143341.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 370.0003 - mse: 365.4006 - val_loss: 401.1433 - val_mse: 396.5386\n",
      "Epoch 22/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 369.0994 - mse: 364.4960\n",
      "Epoch 22: val_loss improved from 401.14334 to 400.73398, saving model to music_timbre_regression\\model.00022_400.733978.h5\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 369.0994 - mse: 364.4960 - val_loss: 400.7340 - val_mse: 396.1311\n",
      "Epoch 23/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 366.3323 - mse: 361.7266\n",
      "Epoch 23: val_loss did not improve from 400.73398\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 366.3323 - mse: 361.7266 - val_loss: 405.6699 - val_mse: 401.0685\n",
      "Epoch 24/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 363.9977 - mse: 359.3948\n",
      "Epoch 24: val_loss improved from 400.73398 to 398.84351, saving model to music_timbre_regression\\model.00024_398.843506.h5\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 363.9977 - mse: 359.3948 - val_loss: 398.8435 - val_mse: 394.2374\n",
      "Epoch 25/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 361.0686 - mse: 356.4595\n",
      "Epoch 25: val_loss did not improve from 398.84351\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 361.0686 - mse: 356.4595 - val_loss: 403.2836 - val_mse: 398.6704\n",
      "Epoch 26/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 362.1082 - mse: 357.5004\n",
      "Epoch 26: val_loss did not improve from 398.84351\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 362.1082 - mse: 357.5004 - val_loss: 405.7360 - val_mse: 401.1346\n",
      "Epoch 27/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 358.4838 - mse: 353.8807\n",
      "Epoch 27: val_loss improved from 398.84351 to 395.91467, saving model to music_timbre_regression\\model.00027_395.914673.h5\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 358.5179 - mse: 353.9148 - val_loss: 395.9147 - val_mse: 391.3082\n",
      "Epoch 28/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 357.0739 - mse: 352.4667\n",
      "Epoch 28: val_loss improved from 395.91467 to 394.64816, saving model to music_timbre_regression\\model.00028_394.648163.h5\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 357.0739 - mse: 352.4667 - val_loss: 394.6482 - val_mse: 390.0393\n",
      "Epoch 29/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 356.2066 - mse: 351.5980\n",
      "Epoch 29: val_loss did not improve from 394.64816\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 356.2066 - mse: 351.5980 - val_loss: 399.7238 - val_mse: 395.1147\n",
      "Epoch 30/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 354.1154 - mse: 349.5042\n",
      "Epoch 30: val_loss improved from 394.64816 to 393.14795, saving model to music_timbre_regression\\model.00030_393.147949.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 354.1154 - mse: 349.5042 - val_loss: 393.1479 - val_mse: 388.5362\n",
      "Epoch 31/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 353.2126 - mse: 348.6023\n",
      "Epoch 31: val_loss did not improve from 393.14795\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 353.2126 - mse: 348.6023 - val_loss: 396.0851 - val_mse: 391.4746\n",
      "Epoch 32/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 351.3542 - mse: 346.7410\n",
      "Epoch 32: val_loss did not improve from 393.14795\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 351.2943 - mse: 346.6812 - val_loss: 396.2461 - val_mse: 391.6374\n",
      "Epoch 33/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 350.8992 - mse: 346.2957\n",
      "Epoch 33: val_loss did not improve from 393.14795\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 350.8449 - mse: 346.2414 - val_loss: 393.2518 - val_mse: 388.6512\n",
      "Epoch 34/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 348.9594 - mse: 344.3567\n",
      "Epoch 34: val_loss improved from 393.14795 to 387.82837, saving model to music_timbre_regression\\model.00034_387.828369.h5\n",
      "85/85 [==============================] - 5s 62ms/step - loss: 348.9589 - mse: 344.3562 - val_loss: 387.8284 - val_mse: 383.2219\n",
      "Epoch 35/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 348.8569 - mse: 344.2544\n",
      "Epoch 35: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 5s 62ms/step - loss: 348.8569 - mse: 344.2544 - val_loss: 395.5652 - val_mse: 390.9677\n",
      "Epoch 36/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 347.2162 - mse: 342.6169\n",
      "Epoch 36: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 347.3142 - mse: 342.7150 - val_loss: 405.0108 - val_mse: 400.4151\n",
      "Epoch 37/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 345.6174 - mse: 341.0179\n",
      "Epoch 37: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 345.6174 - mse: 341.0179 - val_loss: 394.5889 - val_mse: 389.9856\n",
      "Epoch 38/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 344.5691 - mse: 339.9674\n",
      "Epoch 38: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 344.5842 - mse: 339.9825 - val_loss: 390.1375 - val_mse: 385.5287\n",
      "Epoch 39/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 343.6984 - mse: 339.0886\n",
      "Epoch 39: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 343.6217 - mse: 339.0119 - val_loss: 387.8743 - val_mse: 383.2643\n",
      "Epoch 40/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 343.7404 - mse: 339.1378\n",
      "Epoch 40: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 343.7404 - mse: 339.1378 - val_loss: 402.3477 - val_mse: 397.7490\n",
      "Epoch 41/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 340.9603 - mse: 336.3580\n",
      "Epoch 41: val_loss did not improve from 387.82837\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 341.1436 - mse: 336.5412 - val_loss: 390.7266 - val_mse: 386.1214\n",
      "Epoch 42/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 344.5212 - mse: 339.9295\n",
      "Epoch 42: val_loss improved from 387.82837 to 387.61356, saving model to music_timbre_regression\\model.00042_387.613556.h5\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 344.3709 - mse: 339.7794 - val_loss: 387.6136 - val_mse: 383.0385\n",
      "Epoch 43/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 339.3321 - mse: 334.7468\n",
      "Epoch 43: val_loss did not improve from 387.61356\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 339.3321 - mse: 334.7468 - val_loss: 391.2066 - val_mse: 386.6129\n",
      "Epoch 44/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 339.0052 - mse: 334.4128\n",
      "Epoch 44: val_loss did not improve from 387.61356\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 339.0052 - mse: 334.4128 - val_loss: 388.6970 - val_mse: 384.1047\n",
      "Epoch 45/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 338.0195 - mse: 333.4256\n",
      "Epoch 45: val_loss improved from 387.61356 to 385.14758, saving model to music_timbre_regression\\model.00045_385.147583.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 338.0195 - mse: 333.4256 - val_loss: 385.1476 - val_mse: 380.5540\n",
      "Epoch 46/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 338.8771 - mse: 334.2919\n",
      "Epoch 46: val_loss did not improve from 385.14758\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 338.8771 - mse: 334.2919 - val_loss: 398.0436 - val_mse: 393.4653\n",
      "Epoch 47/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 337.5932 - mse: 333.0175\n",
      "Epoch 47: val_loss improved from 385.14758 to 382.63724, saving model to music_timbre_regression\\model.00047_382.637238.h5\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 337.5932 - mse: 333.0175 - val_loss: 382.6372 - val_mse: 378.0617\n",
      "Epoch 48/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 337.2117 - mse: 332.6373\n",
      "Epoch 48: val_loss did not improve from 382.63724\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 337.0567 - mse: 332.4823 - val_loss: 385.2984 - val_mse: 380.7277\n",
      "Epoch 49/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 335.0955 - mse: 330.5220\n",
      "Epoch 49: val_loss did not improve from 382.63724\n",
      "85/85 [==============================] - 4s 50ms/step - loss: 335.1503 - mse: 330.5767 - val_loss: 385.1363 - val_mse: 380.5602\n",
      "Epoch 50/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 335.8212 - mse: 331.2538\n",
      "Epoch 50: val_loss did not improve from 382.63724\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 335.8212 - mse: 331.2538 - val_loss: 387.2902 - val_mse: 382.7227\n",
      "Epoch 51/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 334.0885 - mse: 329.5204\n",
      "Epoch 51: val_loss did not improve from 382.63724\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 334.0885 - mse: 329.5204 - val_loss: 382.7572 - val_mse: 378.1860\n",
      "Epoch 52/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 334.1446 - mse: 329.5775\n",
      "Epoch 52: val_loss improved from 382.63724 to 382.49985, saving model to music_timbre_regression\\model.00052_382.499847.h5\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 334.1796 - mse: 329.6125 - val_loss: 382.4998 - val_mse: 377.9329\n",
      "Epoch 53/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 332.6406 - mse: 328.0732\n",
      "Epoch 53: val_loss did not improve from 382.49985\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 332.8590 - mse: 328.2916 - val_loss: 386.5513 - val_mse: 381.9849\n",
      "Epoch 54/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 331.9648 - mse: 327.3974\n",
      "Epoch 54: val_loss improved from 382.49985 to 382.00024, saving model to music_timbre_regression\\model.00054_382.000244.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 331.9648 - mse: 327.3974 - val_loss: 382.0002 - val_mse: 377.4313\n",
      "Epoch 55/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 331.9859 - mse: 327.4189\n",
      "Epoch 55: val_loss improved from 382.00024 to 378.88455, saving model to music_timbre_regression\\model.00055_378.884552.h5\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 331.8036 - mse: 327.2366 - val_loss: 378.8846 - val_mse: 374.3192\n",
      "Epoch 56/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 330.6166 - mse: 326.0531\n",
      "Epoch 56: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 330.6166 - mse: 326.0531 - val_loss: 382.8266 - val_mse: 378.2605\n",
      "Epoch 57/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 329.6779 - mse: 325.1096\n",
      "Epoch 57: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 329.5890 - mse: 325.0207 - val_loss: 392.6779 - val_mse: 388.1149\n",
      "Epoch 58/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 329.7649 - mse: 325.2101\n",
      "Epoch 58: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 329.7649 - mse: 325.2101 - val_loss: 379.0055 - val_mse: 374.4511\n",
      "Epoch 59/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 330.4014 - mse: 325.8462\n",
      "Epoch 59: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 330.2118 - mse: 325.6567 - val_loss: 388.0860 - val_mse: 383.5451\n",
      "Epoch 60/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 327.6864 - mse: 323.1427\n",
      "Epoch 60: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 327.6864 - mse: 323.1427 - val_loss: 380.7614 - val_mse: 376.2151\n",
      "Epoch 61/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 326.9210 - mse: 322.3749\n",
      "Epoch 61: val_loss did not improve from 378.88455\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 327.0630 - mse: 322.5170 - val_loss: 380.2181 - val_mse: 375.6751\n",
      "Epoch 62/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 326.9859 - mse: 322.4476\n",
      "Epoch 62: val_loss improved from 378.88455 to 373.12039, saving model to music_timbre_regression\\model.00062_373.120392.h5\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 326.9859 - mse: 322.4476 - val_loss: 373.1204 - val_mse: 368.5849\n",
      "Epoch 63/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 325.6139 - mse: 321.0806\n",
      "Epoch 63: val_loss did not improve from 373.12039\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 325.6139 - mse: 321.0806 - val_loss: 374.3196 - val_mse: 369.7871\n",
      "Epoch 64/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 324.9122 - mse: 320.3804\n",
      "Epoch 64: val_loss did not improve from 373.12039\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 324.9122 - mse: 320.3804 - val_loss: 379.4416 - val_mse: 374.9123\n",
      "Epoch 65/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 325.3289 - mse: 320.8033\n",
      "Epoch 65: val_loss did not improve from 373.12039\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 325.3454 - mse: 320.8199 - val_loss: 380.6021 - val_mse: 376.0801\n",
      "Epoch 66/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 324.2261 - mse: 319.7062\n",
      "Epoch 66: val_loss did not improve from 373.12039\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 324.2261 - mse: 319.7062 - val_loss: 380.4935 - val_mse: 375.9747\n",
      "Epoch 67/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 324.4912 - mse: 319.9790\n",
      "Epoch 67: val_loss improved from 373.12039 to 371.10172, saving model to music_timbre_regression\\model.00067_371.101715.h5\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 324.4654 - mse: 319.9532 - val_loss: 371.1017 - val_mse: 366.5917\n",
      "Epoch 68/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 323.9420 - mse: 319.4310\n",
      "Epoch 68: val_loss did not improve from 371.10172\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 323.9984 - mse: 319.4875 - val_loss: 373.4014 - val_mse: 368.9010\n",
      "Epoch 69/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 323.8648 - mse: 319.3648\n",
      "Epoch 69: val_loss improved from 371.10172 to 369.51407, saving model to music_timbre_regression\\model.00069_369.514069.h5\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 323.7682 - mse: 319.2683 - val_loss: 369.5141 - val_mse: 365.0203\n",
      "Epoch 70/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 322.2317 - mse: 317.7346\n",
      "Epoch 70: val_loss did not improve from 369.51407\n",
      "85/85 [==============================] - 5s 58ms/step - loss: 322.1752 - mse: 317.6781 - val_loss: 374.1556 - val_mse: 369.6595\n",
      "Epoch 71/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 321.7881 - mse: 317.2923\n",
      "Epoch 71: val_loss improved from 369.51407 to 369.50684, saving model to music_timbre_regression\\model.00071_369.506836.h5\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 321.8622 - mse: 317.3664 - val_loss: 369.5068 - val_mse: 365.0127\n",
      "Epoch 72/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 322.3699 - mse: 317.8808\n",
      "Epoch 72: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 322.3699 - mse: 317.8808 - val_loss: 386.2965 - val_mse: 381.8185\n",
      "Epoch 73/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 322.0264 - mse: 317.5489\n",
      "Epoch 73: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 322.0264 - mse: 317.5489 - val_loss: 373.8267 - val_mse: 369.3489\n",
      "Epoch 74/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 321.1980 - mse: 316.7203\n",
      "Epoch 74: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 321.1980 - mse: 316.7203 - val_loss: 376.1073 - val_mse: 371.6328\n",
      "Epoch 75/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 321.0647 - mse: 316.5947\n",
      "Epoch 75: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 321.0647 - mse: 316.5947 - val_loss: 376.8971 - val_mse: 372.4289\n",
      "Epoch 76/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 320.5522 - mse: 316.0841\n",
      "Epoch 76: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 320.5500 - mse: 316.0820 - val_loss: 369.8247 - val_mse: 365.3650\n",
      "Epoch 77/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 319.9742 - mse: 315.5119\n",
      "Epoch 77: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 319.8659 - mse: 315.4036 - val_loss: 370.1080 - val_mse: 365.6491\n",
      "Epoch 78/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 320.2191 - mse: 315.7655\n",
      "Epoch 78: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 320.1850 - mse: 315.7315 - val_loss: 377.2486 - val_mse: 372.7994\n",
      "Epoch 79/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 319.9565 - mse: 315.5123\n",
      "Epoch 79: val_loss did not improve from 369.50684\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 319.8322 - mse: 315.3880 - val_loss: 374.6646 - val_mse: 370.2230\n",
      "Epoch 80/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 318.8050 - mse: 314.3633\n",
      "Epoch 80: val_loss improved from 369.50684 to 367.79526, saving model to music_timbre_regression\\model.00080_367.795258.h5\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 318.8050 - mse: 314.3633 - val_loss: 367.7953 - val_mse: 363.3555\n",
      "Epoch 81/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 319.2159 - mse: 314.7786\n",
      "Epoch 81: val_loss did not improve from 367.79526\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 319.2414 - mse: 314.8041 - val_loss: 369.7998 - val_mse: 365.3698\n",
      "Epoch 82/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 318.0494 - mse: 313.6186\n",
      "Epoch 82: val_loss improved from 367.79526 to 367.07562, saving model to music_timbre_regression\\model.00082_367.075623.h5\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 317.9080 - mse: 313.4771 - val_loss: 367.0756 - val_mse: 362.6456\n",
      "Epoch 83/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 319.0184 - mse: 314.5895\n",
      "Epoch 83: val_loss did not improve from 367.07562\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 319.0602 - mse: 314.6314 - val_loss: 367.1907 - val_mse: 362.7734\n",
      "Epoch 84/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 318.6711 - mse: 314.2586\n",
      "Epoch 84: val_loss did not improve from 367.07562\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 318.6711 - mse: 314.2586 - val_loss: 368.2449 - val_mse: 363.8358\n",
      "Epoch 85/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 318.4565 - mse: 314.0511\n",
      "Epoch 85: val_loss did not improve from 367.07562\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 318.5003 - mse: 314.0948 - val_loss: 369.0754 - val_mse: 364.6722\n",
      "Epoch 86/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 316.9851 - mse: 312.5828\n",
      "Epoch 86: val_loss did not improve from 367.07562\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 317.1440 - mse: 312.7417 - val_loss: 374.7866 - val_mse: 370.3836\n",
      "Epoch 87/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 317.4629 - mse: 313.0633\n",
      "Epoch 87: val_loss did not improve from 367.07562\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 317.4629 - mse: 313.0633 - val_loss: 372.2189 - val_mse: 367.8255\n",
      "Epoch 88/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 317.9836 - mse: 313.5928\n",
      "Epoch 88: val_loss improved from 367.07562 to 364.70468, saving model to music_timbre_regression\\model.00088_364.704681.h5\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 317.9836 - mse: 313.5928 - val_loss: 364.7047 - val_mse: 360.3147\n",
      "Epoch 89/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 316.8582 - mse: 312.4669\n",
      "Epoch 89: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 316.8582 - mse: 312.4669 - val_loss: 369.6714 - val_mse: 365.2831\n",
      "Epoch 90/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 316.8450 - mse: 312.4577\n",
      "Epoch 90: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 316.9129 - mse: 312.5257 - val_loss: 367.2016 - val_mse: 362.8175\n",
      "Epoch 91/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 315.4162 - mse: 311.0291\n",
      "Epoch 91: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 315.4162 - mse: 311.0291 - val_loss: 371.8442 - val_mse: 367.4581\n",
      "Epoch 92/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 315.5045 - mse: 311.1193\n",
      "Epoch 92: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 315.5904 - mse: 311.2052 - val_loss: 367.3459 - val_mse: 362.9625\n",
      "Epoch 93/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 315.9920 - mse: 311.6146\n",
      "Epoch 93: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 315.9920 - mse: 311.6146 - val_loss: 366.1227 - val_mse: 361.7514\n",
      "Epoch 94/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 315.3221 - mse: 310.9518\n",
      "Epoch 94: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 315.3221 - mse: 310.9518 - val_loss: 367.3286 - val_mse: 362.9615\n",
      "Epoch 95/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 314.5367 - mse: 310.1709\n",
      "Epoch 95: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 314.5367 - mse: 310.1709 - val_loss: 368.4916 - val_mse: 364.1243\n",
      "Epoch 96/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 316.4220 - mse: 312.0646\n",
      "Epoch 96: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 316.3511 - mse: 311.9938 - val_loss: 374.3752 - val_mse: 370.0234\n",
      "Epoch 97/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 314.3220 - mse: 309.9695\n",
      "Epoch 97: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 314.2911 - mse: 309.9385 - val_loss: 367.1073 - val_mse: 362.7495\n",
      "Epoch 98/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 313.8237 - mse: 309.4652\n",
      "Epoch 98: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 5s 60ms/step - loss: 313.8287 - mse: 309.4703 - val_loss: 374.9152 - val_mse: 370.5593\n",
      "Epoch 99/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 314.3948 - mse: 310.0421\n",
      "Epoch 99: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 314.3948 - mse: 310.0421 - val_loss: 374.4447 - val_mse: 370.0958\n",
      "Epoch 100/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 314.1201 - mse: 309.7694\n",
      "Epoch 100: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 314.0093 - mse: 309.6586 - val_loss: 370.8667 - val_mse: 366.5232\n",
      "Epoch 101/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 314.4869 - mse: 310.1480\n",
      "Epoch 101: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 314.4133 - mse: 310.0744 - val_loss: 373.0580 - val_mse: 368.7227\n",
      "Epoch 102/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 313.4817 - mse: 309.1450\n",
      "Epoch 102: val_loss did not improve from 364.70468\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 313.5514 - mse: 309.2148 - val_loss: 365.4724 - val_mse: 361.1349\n",
      "Epoch 103/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 313.6034 - mse: 309.2664\n",
      "Epoch 103: val_loss improved from 364.70468 to 364.46783, saving model to music_timbre_regression\\model.00103_364.467834.h5\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 313.6034 - mse: 309.2664 - val_loss: 364.4678 - val_mse: 360.1366\n",
      "Epoch 104/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 315.0206 - mse: 310.6978\n",
      "Epoch 104: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 315.0206 - mse: 310.6978 - val_loss: 369.3748 - val_mse: 365.0527\n",
      "Epoch 105/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 311.7885 - mse: 307.4626\n",
      "Epoch 105: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 311.7456 - mse: 307.4195 - val_loss: 369.3420 - val_mse: 365.0117\n",
      "Epoch 106/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 312.0319 - mse: 307.7012\n",
      "Epoch 106: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 311.9390 - mse: 307.6083 - val_loss: 377.4952 - val_mse: 373.1665\n",
      "Epoch 107/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 313.3671 - mse: 309.0443\n",
      "Epoch 107: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 313.3671 - mse: 309.0443 - val_loss: 373.3754 - val_mse: 369.0598\n",
      "Epoch 108/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 311.8225 - mse: 307.5064\n",
      "Epoch 108: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 311.8225 - mse: 307.5064 - val_loss: 367.5659 - val_mse: 363.2498\n",
      "Epoch 109/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 312.1305 - mse: 307.8171\n",
      "Epoch 109: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 312.1305 - mse: 307.8171 - val_loss: 369.9817 - val_mse: 365.6709\n",
      "Epoch 110/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 311.9402 - mse: 307.6293\n",
      "Epoch 110: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 311.9402 - mse: 307.6293 - val_loss: 374.6303 - val_mse: 370.3240\n",
      "Epoch 111/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 310.7981 - mse: 306.4891\n",
      "Epoch 111: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 51ms/step - loss: 310.7981 - mse: 306.4891 - val_loss: 373.3035 - val_mse: 368.9958\n",
      "Epoch 112/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 311.6524 - mse: 307.3460\n",
      "Epoch 112: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 311.6524 - mse: 307.3460 - val_loss: 366.9262 - val_mse: 362.6227\n",
      "Epoch 113/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 311.3267 - mse: 307.0266\n",
      "Epoch 113: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 311.1870 - mse: 306.8868 - val_loss: 371.6895 - val_mse: 367.3890\n",
      "Epoch 114/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 311.1035 - mse: 306.8037\n",
      "Epoch 114: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 311.0638 - mse: 306.7640 - val_loss: 369.8629 - val_mse: 365.5639\n",
      "Epoch 115/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 311.6586 - mse: 307.3649\n",
      "Epoch 115: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 311.6586 - mse: 307.3649 - val_loss: 377.8857 - val_mse: 373.5939\n",
      "Epoch 116/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 310.9597 - mse: 306.6690\n",
      "Epoch 116: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 310.9597 - mse: 306.6690 - val_loss: 374.0085 - val_mse: 369.7162\n",
      "Epoch 117/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 310.7516 - mse: 306.4616\n",
      "Epoch 117: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 310.7591 - mse: 306.4691 - val_loss: 364.5856 - val_mse: 360.2967\n",
      "Epoch 118/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 310.8160 - mse: 306.5325\n",
      "Epoch 118: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 310.8160 - mse: 306.5325 - val_loss: 375.2328 - val_mse: 370.9499\n",
      "Epoch 119/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 309.5978 - mse: 305.3153\n",
      "Epoch 119: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 309.6557 - mse: 305.3732 - val_loss: 374.9517 - val_mse: 370.6665\n",
      "Epoch 120/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 310.5407 - mse: 306.2597\n",
      "Epoch 120: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 310.5407 - mse: 306.2597 - val_loss: 372.7751 - val_mse: 368.4962\n",
      "Epoch 121/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 309.4982 - mse: 305.2187\n",
      "Epoch 121: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 309.4982 - mse: 305.2187 - val_loss: 374.3944 - val_mse: 370.1181\n",
      "Epoch 122/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 308.9282 - mse: 304.6495\n",
      "Epoch 122: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 309.0121 - mse: 304.7334 - val_loss: 372.6656 - val_mse: 368.3863\n",
      "Epoch 123/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 310.3287 - mse: 306.0563\n",
      "Epoch 123: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 310.3287 - mse: 306.0563 - val_loss: 370.4048 - val_mse: 366.1342\n",
      "Epoch 124/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 309.7458 - mse: 305.4760\n",
      "Epoch 124: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 309.7908 - mse: 305.5211 - val_loss: 386.1141 - val_mse: 381.8486\n",
      "Epoch 125/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 309.2061 - mse: 304.9429\n",
      "Epoch 125: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 309.2061 - mse: 304.9429 - val_loss: 376.8971 - val_mse: 372.6353\n",
      "Epoch 126/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.9000 - mse: 304.6351\n",
      "Epoch 126: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 308.9000 - mse: 304.6351 - val_loss: 369.4662 - val_mse: 365.2012\n",
      "Epoch 127/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.8007 - mse: 304.5385\n",
      "Epoch 127: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.8007 - mse: 304.5385 - val_loss: 384.8216 - val_mse: 380.5609\n",
      "Epoch 128/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.3480 - mse: 304.0865\n",
      "Epoch 128: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.3480 - mse: 304.0865 - val_loss: 376.1716 - val_mse: 371.9144\n",
      "Epoch 129/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.5740 - mse: 304.3195\n",
      "Epoch 129: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.5740 - mse: 304.3195 - val_loss: 375.7850 - val_mse: 371.5292\n",
      "Epoch 130/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.4504 - mse: 304.1978\n",
      "Epoch 130: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.4504 - mse: 304.1978 - val_loss: 374.6662 - val_mse: 370.4161\n",
      "Epoch 131/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 308.3194 - mse: 304.0718\n",
      "Epoch 131: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 308.3159 - mse: 304.0682 - val_loss: 367.7336 - val_mse: 363.4859\n",
      "Epoch 132/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.5213 - mse: 304.2783\n",
      "Epoch 132: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.5213 - mse: 304.2783 - val_loss: 375.4625 - val_mse: 371.2215\n",
      "Epoch 133/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 308.2328 - mse: 303.9946\n",
      "Epoch 133: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 308.2328 - mse: 303.9946 - val_loss: 380.7386 - val_mse: 376.5036\n",
      "Epoch 134/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 307.1475 - mse: 302.9114\n",
      "Epoch 134: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 307.3927 - mse: 303.1566 - val_loss: 381.2965 - val_mse: 377.0596\n",
      "Epoch 135/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 309.3408 - mse: 305.1094\n",
      "Epoch 135: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 309.2803 - mse: 305.0490 - val_loss: 382.3582 - val_mse: 378.1322\n",
      "Epoch 136/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 307.9106 - mse: 303.6843\n",
      "Epoch 136: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 307.8932 - mse: 303.6669 - val_loss: 374.9426 - val_mse: 370.7139\n",
      "Epoch 137/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 306.8500 - mse: 302.6200\n",
      "Epoch 137: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.8500 - mse: 302.6200 - val_loss: 383.0264 - val_mse: 378.7983\n",
      "Epoch 138/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 306.8856 - mse: 302.6541\n",
      "Epoch 138: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.8856 - mse: 302.6541 - val_loss: 375.4396 - val_mse: 371.2059\n",
      "Epoch 139/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 306.7331 - mse: 302.4997\n",
      "Epoch 139: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.8702 - mse: 302.6368 - val_loss: 388.6250 - val_mse: 384.3924\n",
      "Epoch 140/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 307.5633 - mse: 303.3347\n",
      "Epoch 140: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 307.5633 - mse: 303.3347 - val_loss: 381.1084 - val_mse: 376.8830\n",
      "Epoch 141/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 306.9112 - mse: 302.6877\n",
      "Epoch 141: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 306.9112 - mse: 302.6877 - val_loss: 378.8142 - val_mse: 374.5910\n",
      "Epoch 142/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 307.0276 - mse: 302.8033\n",
      "Epoch 142: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 307.0276 - mse: 302.8033 - val_loss: 377.1729 - val_mse: 372.9541\n",
      "Epoch 143/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 306.5555 - mse: 302.3385\n",
      "Epoch 143: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.4839 - mse: 302.2669 - val_loss: 386.7494 - val_mse: 382.5340\n",
      "Epoch 144/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 306.7645 - mse: 302.5504\n",
      "Epoch 144: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 306.6150 - mse: 302.4010 - val_loss: 383.7996 - val_mse: 379.5856\n",
      "Epoch 145/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 305.9417 - mse: 301.7285\n",
      "Epoch 145: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 305.9220 - mse: 301.7087 - val_loss: 380.1799 - val_mse: 375.9662\n",
      "Epoch 146/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 305.8510 - mse: 301.6363\n",
      "Epoch 146: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 305.7678 - mse: 301.5531 - val_loss: 392.6062 - val_mse: 388.3932\n",
      "Epoch 147/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 307.0427 - mse: 302.8377\n",
      "Epoch 147: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 307.0427 - mse: 302.8377 - val_loss: 388.8308 - val_mse: 384.6284\n",
      "Epoch 148/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 306.4625 - mse: 302.2598\n",
      "Epoch 148: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.4625 - mse: 302.2598 - val_loss: 385.1826 - val_mse: 380.9791\n",
      "Epoch 149/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 306.0075 - mse: 301.8042\n",
      "Epoch 149: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 306.0075 - mse: 301.8042 - val_loss: 384.7495 - val_mse: 380.5465\n",
      "Epoch 150/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 305.4262 - mse: 301.2225\n",
      "Epoch 150: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 305.4262 - mse: 301.2225 - val_loss: 386.4487 - val_mse: 382.2506\n",
      "Epoch 151/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 305.8009 - mse: 301.6062\n",
      "Epoch 151: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 305.8009 - mse: 301.6062 - val_loss: 382.6842 - val_mse: 378.4904\n",
      "Epoch 152/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.9780 - mse: 300.7825\n",
      "Epoch 152: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 304.9780 - mse: 300.7825 - val_loss: 389.5598 - val_mse: 385.3615\n",
      "Epoch 153/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 305.5889 - mse: 301.3911\n",
      "Epoch 153: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 305.6055 - mse: 301.4078 - val_loss: 393.1266 - val_mse: 388.9303\n",
      "Epoch 154/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 304.7523 - mse: 300.5543\n",
      "Epoch 154: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 304.7547 - mse: 300.5568 - val_loss: 388.6119 - val_mse: 384.4156\n",
      "Epoch 155/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 305.5458 - mse: 301.3535\n",
      "Epoch 155: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 305.5458 - mse: 301.3535 - val_loss: 382.5890 - val_mse: 378.3987\n",
      "Epoch 156/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 304.8016 - mse: 300.6154\n",
      "Epoch 156: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 61ms/step - loss: 304.9031 - mse: 300.7169 - val_loss: 388.4301 - val_mse: 384.2452\n",
      "Epoch 157/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 305.1472 - mse: 300.9611\n",
      "Epoch 157: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 305.0900 - mse: 300.9039 - val_loss: 388.2296 - val_mse: 384.0477\n",
      "Epoch 158/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.8168 - mse: 300.6378\n",
      "Epoch 158: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 304.8168 - mse: 300.6378 - val_loss: 388.5012 - val_mse: 384.3210\n",
      "Epoch 159/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 305.4270 - mse: 301.2490\n",
      "Epoch 159: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 305.3624 - mse: 301.1844 - val_loss: 382.2055 - val_mse: 378.0333\n",
      "Epoch 160/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 304.6483 - mse: 300.4764\n",
      "Epoch 160: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 304.6606 - mse: 300.4887 - val_loss: 385.4384 - val_mse: 381.2640\n",
      "Epoch 161/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.5933 - mse: 300.4182\n",
      "Epoch 161: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 304.5933 - mse: 300.4182 - val_loss: 389.2851 - val_mse: 385.1106\n",
      "Epoch 162/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 304.1680 - mse: 299.9964\n",
      "Epoch 162: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 304.3425 - mse: 300.1708 - val_loss: 390.1974 - val_mse: 386.0229\n",
      "Epoch 163/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.5340 - mse: 299.3590\n",
      "Epoch 163: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 303.5012 - mse: 299.3261 - val_loss: 401.6635 - val_mse: 397.4855\n",
      "Epoch 164/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.9256 - mse: 299.7500\n",
      "Epoch 164: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 303.9160 - mse: 299.7404 - val_loss: 377.9664 - val_mse: 373.7893\n",
      "Epoch 165/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.5299 - mse: 300.3581\n",
      "Epoch 165: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 304.5299 - mse: 300.3581 - val_loss: 383.6411 - val_mse: 379.4693\n",
      "Epoch 166/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.5859 - mse: 300.4195\n",
      "Epoch 166: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 304.5859 - mse: 300.4195 - val_loss: 400.2762 - val_mse: 396.1139\n",
      "Epoch 167/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.2763 - mse: 299.1115\n",
      "Epoch 167: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 303.4710 - mse: 299.3061 - val_loss: 399.9580 - val_mse: 395.7903\n",
      "Epoch 168/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.6379 - mse: 299.4723\n",
      "Epoch 168: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 303.6417 - mse: 299.4760 - val_loss: 399.1108 - val_mse: 394.9440\n",
      "Epoch 169/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 304.7507 - mse: 300.5896\n",
      "Epoch 169: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 304.7507 - mse: 300.5896 - val_loss: 394.5645 - val_mse: 390.4048\n",
      "Epoch 170/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.4339 - mse: 299.2729\n",
      "Epoch 170: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 303.3893 - mse: 299.2282 - val_loss: 392.5312 - val_mse: 388.3710\n",
      "Epoch 171/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.4794 - mse: 299.3210\n",
      "Epoch 171: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 303.4568 - mse: 299.2984 - val_loss: 390.4647 - val_mse: 386.3059\n",
      "Epoch 172/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.7889 - mse: 298.6277\n",
      "Epoch 172: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 302.8409 - mse: 298.6797 - val_loss: 384.6141 - val_mse: 380.4522\n",
      "Epoch 173/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 303.2449 - mse: 299.0840\n",
      "Epoch 173: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 303.2449 - mse: 299.0840 - val_loss: 397.1864 - val_mse: 393.0282\n",
      "Epoch 174/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 303.3720 - mse: 299.2142\n",
      "Epoch 174: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 303.3720 - mse: 299.2142 - val_loss: 399.1649 - val_mse: 395.0111\n",
      "Epoch 175/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 303.8866 - mse: 299.7342\n",
      "Epoch 175: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 303.8866 - mse: 299.7342 - val_loss: 400.9861 - val_mse: 396.8368\n",
      "Epoch 176/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.2612 - mse: 299.1108\n",
      "Epoch 176: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 303.1612 - mse: 299.0108 - val_loss: 402.0813 - val_mse: 397.9316\n",
      "Epoch 177/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.0259 - mse: 297.8741\n",
      "Epoch 177: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 301.9229 - mse: 297.7711 - val_loss: 393.0030 - val_mse: 388.8499\n",
      "Epoch 178/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.2600 - mse: 298.1073\n",
      "Epoch 178: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 302.2830 - mse: 298.1302 - val_loss: 391.2751 - val_mse: 387.1204\n",
      "Epoch 179/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 303.9291 - mse: 299.7784\n",
      "Epoch 179: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 303.7853 - mse: 299.6346 - val_loss: 398.8046 - val_mse: 394.6601\n",
      "Epoch 180/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.4641 - mse: 297.3172\n",
      "Epoch 180: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 301.6071 - mse: 297.4602 - val_loss: 406.0690 - val_mse: 401.9205\n",
      "Epoch 181/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 302.4351 - mse: 298.2890\n",
      "Epoch 181: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 302.4351 - mse: 298.2890 - val_loss: 400.5561 - val_mse: 396.4128\n",
      "Epoch 182/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 302.5472 - mse: 298.4062\n",
      "Epoch 182: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 302.5472 - mse: 298.4062 - val_loss: 395.0509 - val_mse: 390.9097\n",
      "Epoch 183/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.8138 - mse: 298.6725\n",
      "Epoch 183: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 302.7538 - mse: 298.6127 - val_loss: 399.2609 - val_mse: 395.1227\n",
      "Epoch 184/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.8517 - mse: 297.7121\n",
      "Epoch 184: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 301.8366 - mse: 297.6971 - val_loss: 398.4855 - val_mse: 394.3452\n",
      "Epoch 185/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 301.2357 - mse: 297.0922\n",
      "Epoch 185: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 301.2357 - mse: 297.0922 - val_loss: 395.3650 - val_mse: 391.2183\n",
      "Epoch 186/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.1891 - mse: 298.0439\n",
      "Epoch 186: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 302.1194 - mse: 297.9742 - val_loss: 394.5168 - val_mse: 390.3730\n",
      "Epoch 187/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.1141 - mse: 296.9695\n",
      "Epoch 187: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 301.1206 - mse: 296.9760 - val_loss: 403.9865 - val_mse: 399.8390\n",
      "Epoch 188/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 301.5870 - mse: 297.4416\n",
      "Epoch 188: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 301.5870 - mse: 297.4416 - val_loss: 412.2372 - val_mse: 408.0942\n",
      "Epoch 189/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 300.8228 - mse: 296.6795\n",
      "Epoch 189: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 300.8228 - mse: 296.6795 - val_loss: 407.1827 - val_mse: 403.0365\n",
      "Epoch 190/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.2327 - mse: 298.0918\n",
      "Epoch 190: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 302.2054 - mse: 298.0645 - val_loss: 409.3626 - val_mse: 405.2242\n",
      "Epoch 191/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.9843 - mse: 297.8453\n",
      "Epoch 191: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 301.9525 - mse: 297.8135 - val_loss: 399.9631 - val_mse: 395.8288\n",
      "Epoch 192/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.9311 - mse: 297.7978\n",
      "Epoch 192: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 301.8481 - mse: 297.7147 - val_loss: 399.3779 - val_mse: 395.2445\n",
      "Epoch 193/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 301.5336 - mse: 297.3987\n",
      "Epoch 193: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 301.5336 - mse: 297.3987 - val_loss: 399.8845 - val_mse: 395.7530\n",
      "Epoch 194/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.1534 - mse: 297.0202\n",
      "Epoch 194: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 58ms/step - loss: 301.1263 - mse: 296.9932 - val_loss: 409.4117 - val_mse: 405.2775\n",
      "Epoch 195/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 302.2163 - mse: 298.0833\n",
      "Epoch 195: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 302.2579 - mse: 298.1249 - val_loss: 401.7457 - val_mse: 397.6193\n",
      "Epoch 196/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 301.3708 - mse: 297.2449\n",
      "Epoch 196: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 301.3708 - mse: 297.2449 - val_loss: 396.2304 - val_mse: 392.0985\n",
      "Epoch 197/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 300.7108 - mse: 296.5783\n",
      "Epoch 197: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 300.7108 - mse: 296.5783 - val_loss: 403.1535 - val_mse: 399.0218\n",
      "Epoch 198/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.7294 - mse: 295.5947\n",
      "Epoch 198: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 62ms/step - loss: 299.6906 - mse: 295.5558 - val_loss: 416.1117 - val_mse: 411.9752\n",
      "Epoch 199/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.3231 - mse: 297.1902\n",
      "Epoch 199: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 62ms/step - loss: 301.3881 - mse: 297.2552 - val_loss: 397.4671 - val_mse: 393.3356\n",
      "Epoch 200/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.2386 - mse: 297.1095\n",
      "Epoch 200: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 60ms/step - loss: 301.3008 - mse: 297.1716 - val_loss: 394.9649 - val_mse: 390.8361\n",
      "Epoch 201/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.7572 - mse: 297.6315\n",
      "Epoch 201: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 301.6535 - mse: 297.5278 - val_loss: 415.3803 - val_mse: 411.2586\n",
      "Epoch 202/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 300.3083 - mse: 296.1827\n",
      "Epoch 202: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 300.1997 - mse: 296.0742 - val_loss: 408.6812 - val_mse: 404.5531\n",
      "Epoch 203/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.4897 - mse: 295.3590\n",
      "Epoch 203: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 58ms/step - loss: 299.4897 - mse: 295.3590 - val_loss: 407.0426 - val_mse: 402.9080\n",
      "Epoch 204/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 300.1297 - mse: 295.9956\n",
      "Epoch 204: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 60ms/step - loss: 300.1358 - mse: 296.0017 - val_loss: 426.5211 - val_mse: 422.3865\n",
      "Epoch 205/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 300.0335 - mse: 295.9006\n",
      "Epoch 205: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 300.0335 - mse: 295.9006 - val_loss: 406.5222 - val_mse: 402.3883\n",
      "Epoch 206/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.1788 - mse: 295.0449\n",
      "Epoch 206: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 299.1788 - mse: 295.0449 - val_loss: 427.4512 - val_mse: 423.3171\n",
      "Epoch 207/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 301.3411 - mse: 297.2117\n",
      "Epoch 207: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 301.3953 - mse: 297.2659 - val_loss: 400.5548 - val_mse: 396.4283\n",
      "Epoch 208/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 300.0132 - mse: 295.8873\n",
      "Epoch 208: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 300.0132 - mse: 295.8873 - val_loss: 407.5583 - val_mse: 403.4314\n",
      "Epoch 209/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.2724 - mse: 295.1451\n",
      "Epoch 209: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 299.3149 - mse: 295.1875 - val_loss: 415.6750 - val_mse: 411.5451\n",
      "Epoch 210/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 300.7320 - mse: 296.6063\n",
      "Epoch 210: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 300.6364 - mse: 296.5107 - val_loss: 416.8728 - val_mse: 412.7466\n",
      "Epoch 211/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.8559 - mse: 295.7300\n",
      "Epoch 211: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 299.8559 - mse: 295.7300 - val_loss: 403.1730 - val_mse: 399.0474\n",
      "Epoch 212/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.1331 - mse: 295.0066\n",
      "Epoch 212: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 299.1331 - mse: 295.0066 - val_loss: 422.6019 - val_mse: 418.4756\n",
      "Epoch 213/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.6222 - mse: 295.4971\n",
      "Epoch 213: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 299.4580 - mse: 295.3329 - val_loss: 418.3502 - val_mse: 414.2240\n",
      "Epoch 214/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.3903 - mse: 295.2639\n",
      "Epoch 214: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 299.4236 - mse: 295.2972 - val_loss: 408.2888 - val_mse: 404.1635\n",
      "Epoch 215/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 298.7312 - mse: 294.6064\n",
      "Epoch 215: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 298.8359 - mse: 294.7110 - val_loss: 409.8593 - val_mse: 405.7356\n",
      "Epoch 216/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.8685 - mse: 294.7437\n",
      "Epoch 216: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 298.8685 - mse: 294.7437 - val_loss: 418.8651 - val_mse: 414.7432\n",
      "Epoch 217/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.3394 - mse: 295.2197\n",
      "Epoch 217: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 299.3394 - mse: 295.2197 - val_loss: 421.6306 - val_mse: 417.5109\n",
      "Epoch 218/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 299.3788 - mse: 295.2596\n",
      "Epoch 218: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 299.3788 - mse: 295.2596 - val_loss: 417.0458 - val_mse: 412.9233\n",
      "Epoch 219/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.6564 - mse: 294.5336\n",
      "Epoch 219: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 298.6564 - mse: 294.5336 - val_loss: 419.1927 - val_mse: 415.0655\n",
      "Epoch 220/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.3293 - mse: 294.2024\n",
      "Epoch 220: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 298.3293 - mse: 294.2024 - val_loss: 420.5464 - val_mse: 416.4227\n",
      "Epoch 221/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.0486 - mse: 294.9231\n",
      "Epoch 221: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 299.1134 - mse: 294.9879 - val_loss: 428.6924 - val_mse: 424.5719\n",
      "Epoch 222/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.7620 - mse: 294.6389\n",
      "Epoch 222: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 298.7620 - mse: 294.6389 - val_loss: 418.0035 - val_mse: 413.8812\n",
      "Epoch 223/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 298.6588 - mse: 294.5373\n",
      "Epoch 223: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 298.7533 - mse: 294.6318 - val_loss: 417.4301 - val_mse: 413.3111\n",
      "Epoch 224/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.6417 - mse: 294.5242\n",
      "Epoch 224: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 298.6417 - mse: 294.5242 - val_loss: 417.0226 - val_mse: 412.9043\n",
      "Epoch 225/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.8910 - mse: 293.7721\n",
      "Epoch 225: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 297.9280 - mse: 293.8091 - val_loss: 427.4892 - val_mse: 423.3690\n",
      "Epoch 226/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.5784 - mse: 293.4565\n",
      "Epoch 226: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 297.6615 - mse: 293.5396 - val_loss: 424.3430 - val_mse: 420.2202\n",
      "Epoch 227/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.7074 - mse: 293.5846\n",
      "Epoch 227: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 297.7472 - mse: 293.6244 - val_loss: 413.2379 - val_mse: 409.1168\n",
      "Epoch 228/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 299.4853 - mse: 295.3679\n",
      "Epoch 228: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 60ms/step - loss: 299.3124 - mse: 295.1950 - val_loss: 424.4889 - val_mse: 420.3734\n",
      "Epoch 229/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.0614 - mse: 293.9465\n",
      "Epoch 229: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 298.0614 - mse: 293.9465 - val_loss: 435.8774 - val_mse: 431.7636\n",
      "Epoch 230/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 298.7867 - mse: 294.6725\n",
      "Epoch 230: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 298.6620 - mse: 294.5479 - val_loss: 422.9213 - val_mse: 418.8086\n",
      "Epoch 231/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.4224 - mse: 293.3062\n",
      "Epoch 231: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.4740 - mse: 293.3577 - val_loss: 427.9850 - val_mse: 423.8658\n",
      "Epoch 232/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 298.7699 - mse: 294.6517\n",
      "Epoch 232: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 298.7699 - mse: 294.6517 - val_loss: 437.1359 - val_mse: 433.0192\n",
      "Epoch 233/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.2328 - mse: 293.1128\n",
      "Epoch 233: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.3284 - mse: 293.2085 - val_loss: 413.5373 - val_mse: 409.4150\n",
      "Epoch 234/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 298.3123 - mse: 294.1927\n",
      "Epoch 234: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 298.2676 - mse: 294.1481 - val_loss: 434.3117 - val_mse: 430.1921\n",
      "Epoch 235/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 297.5286 - mse: 293.4077\n",
      "Epoch 235: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 297.5286 - mse: 293.4077 - val_loss: 418.9795 - val_mse: 414.8589\n",
      "Epoch 236/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.9271 - mse: 293.8093\n",
      "Epoch 236: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.9290 - mse: 293.8111 - val_loss: 445.4582 - val_mse: 441.3412\n",
      "Epoch 237/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.9904 - mse: 293.8751\n",
      "Epoch 237: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 298.1367 - mse: 294.0214 - val_loss: 435.9370 - val_mse: 431.8198\n",
      "Epoch 238/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.6273 - mse: 293.5104\n",
      "Epoch 238: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.5729 - mse: 293.4561 - val_loss: 428.5437 - val_mse: 424.4295\n",
      "Epoch 239/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.3530 - mse: 293.2388\n",
      "Epoch 239: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 297.4771 - mse: 293.3629 - val_loss: 430.5233 - val_mse: 426.4091\n",
      "Epoch 240/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.3011 - mse: 293.1860\n",
      "Epoch 240: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 297.2213 - mse: 293.1062 - val_loss: 432.1615 - val_mse: 428.0447\n",
      "Epoch 241/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.5248 - mse: 293.4089\n",
      "Epoch 241: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.4470 - mse: 293.3311 - val_loss: 429.7874 - val_mse: 425.6747\n",
      "Epoch 242/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 296.6923 - mse: 292.5772\n",
      "Epoch 242: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 296.6923 - mse: 292.5772 - val_loss: 417.2430 - val_mse: 413.1250\n",
      "Epoch 243/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 297.4747 - mse: 293.3583\n",
      "Epoch 243: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.4747 - mse: 293.3583 - val_loss: 434.0044 - val_mse: 429.8869\n",
      "Epoch 244/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.6558 - mse: 292.5380\n",
      "Epoch 244: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 296.6878 - mse: 292.5700 - val_loss: 433.3537 - val_mse: 429.2362\n",
      "Epoch 245/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.5069 - mse: 293.3905\n",
      "Epoch 245: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 297.4699 - mse: 293.3536 - val_loss: 436.9646 - val_mse: 432.8498\n",
      "Epoch 246/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 296.6140 - mse: 292.4985\n",
      "Epoch 246: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 296.6140 - mse: 292.4985 - val_loss: 435.7061 - val_mse: 431.5898\n",
      "Epoch 247/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.6903 - mse: 292.5742\n",
      "Epoch 247: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 296.5189 - mse: 292.4027 - val_loss: 428.4405 - val_mse: 424.3255\n",
      "Epoch 248/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.2128 - mse: 293.0997\n",
      "Epoch 248: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 297.4470 - mse: 293.3339 - val_loss: 428.7961 - val_mse: 424.6858\n",
      "Epoch 249/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.0959 - mse: 291.9842\n",
      "Epoch 249: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 296.0973 - mse: 291.9855 - val_loss: 429.0147 - val_mse: 424.9015\n",
      "Epoch 250/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 296.9168 - mse: 292.8027\n",
      "Epoch 250: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 296.9168 - mse: 292.8027 - val_loss: 445.1310 - val_mse: 441.0173\n",
      "Epoch 251/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.4353 - mse: 293.3206\n",
      "Epoch 251: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 297.4519 - mse: 293.3372 - val_loss: 429.0538 - val_mse: 424.9420\n",
      "Epoch 252/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.5923 - mse: 291.4757\n",
      "Epoch 252: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 295.6564 - mse: 291.5398 - val_loss: 431.3293 - val_mse: 427.2112\n",
      "Epoch 253/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.8759 - mse: 291.7575\n",
      "Epoch 253: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 295.9587 - mse: 291.8403 - val_loss: 430.0035 - val_mse: 425.8855\n",
      "Epoch 254/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.5065 - mse: 291.3883\n",
      "Epoch 254: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.5065 - mse: 291.3883 - val_loss: 428.0713 - val_mse: 423.9520\n",
      "Epoch 255/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 297.0982 - mse: 292.9826\n",
      "Epoch 255: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 297.0837 - mse: 292.9681 - val_loss: 442.3642 - val_mse: 438.2531\n",
      "Epoch 256/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.8150 - mse: 292.7056\n",
      "Epoch 256: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 296.9109 - mse: 292.8016 - val_loss: 437.5547 - val_mse: 433.4467\n",
      "Epoch 257/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.0451 - mse: 291.9367\n",
      "Epoch 257: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 296.0162 - mse: 291.9078 - val_loss: 449.8594 - val_mse: 445.7510\n",
      "Epoch 258/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.6178 - mse: 292.5077\n",
      "Epoch 258: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 296.3874 - mse: 292.2773 - val_loss: 448.4379 - val_mse: 444.3294\n",
      "Epoch 259/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.6022 - mse: 291.4929\n",
      "Epoch 259: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 52ms/step - loss: 295.8138 - mse: 291.7044 - val_loss: 440.1697 - val_mse: 436.0597\n",
      "Epoch 260/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.7865 - mse: 291.6755\n",
      "Epoch 260: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.7865 - mse: 291.6755 - val_loss: 449.5962 - val_mse: 445.4844\n",
      "Epoch 261/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.3567 - mse: 292.2426\n",
      "Epoch 261: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 296.2729 - mse: 292.1589 - val_loss: 435.6082 - val_mse: 431.4973\n",
      "Epoch 262/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.8891 - mse: 291.7780\n",
      "Epoch 262: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 295.8891 - mse: 291.7780 - val_loss: 439.4308 - val_mse: 435.3196\n",
      "Epoch 263/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.5872 - mse: 291.4750\n",
      "Epoch 263: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 295.6071 - mse: 291.4949 - val_loss: 441.1284 - val_mse: 437.0154\n",
      "Epoch 264/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.0930 - mse: 291.9815\n",
      "Epoch 264: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.9871 - mse: 291.8755 - val_loss: 439.0405 - val_mse: 434.9253\n",
      "Epoch 265/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.3940 - mse: 291.2802\n",
      "Epoch 265: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 295.3940 - mse: 291.2802 - val_loss: 440.7406 - val_mse: 436.6257\n",
      "Epoch 266/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 296.7613 - mse: 292.6477\n",
      "Epoch 266: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 296.8265 - mse: 292.7130 - val_loss: 446.0165 - val_mse: 441.9081\n",
      "Epoch 267/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.8257 - mse: 290.7137\n",
      "Epoch 267: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 294.8387 - mse: 290.7267 - val_loss: 439.9027 - val_mse: 435.7885\n",
      "Epoch 268/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.1879 - mse: 291.0747\n",
      "Epoch 268: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 295.0295 - mse: 290.9163 - val_loss: 443.9302 - val_mse: 439.8141\n",
      "Epoch 269/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.2800 - mse: 291.1655\n",
      "Epoch 269: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 295.3064 - mse: 291.1919 - val_loss: 435.6615 - val_mse: 431.5480\n",
      "Epoch 270/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.6519 - mse: 291.5397\n",
      "Epoch 270: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 295.6190 - mse: 291.5068 - val_loss: 443.1451 - val_mse: 439.0365\n",
      "Epoch 271/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.5382 - mse: 291.4271\n",
      "Epoch 271: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.5382 - mse: 291.4271 - val_loss: 448.3657 - val_mse: 444.2563\n",
      "Epoch 272/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.2804 - mse: 291.1714\n",
      "Epoch 272: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.2804 - mse: 291.1714 - val_loss: 436.0272 - val_mse: 431.9158\n",
      "Epoch 273/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 294.6037 - mse: 290.4901\n",
      "Epoch 273: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 294.6037 - mse: 290.4901 - val_loss: 445.3287 - val_mse: 441.2122\n",
      "Epoch 274/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.4177 - mse: 291.3024\n",
      "Epoch 274: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 295.4177 - mse: 291.3024 - val_loss: 449.0994 - val_mse: 444.9857\n",
      "Epoch 275/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.0717 - mse: 290.9583\n",
      "Epoch 275: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 61ms/step - loss: 295.0159 - mse: 290.9023 - val_loss: 435.2713 - val_mse: 431.1565\n",
      "Epoch 276/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 294.5884 - mse: 290.4741\n",
      "Epoch 276: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 294.5884 - mse: 290.4741 - val_loss: 438.1005 - val_mse: 433.9862\n",
      "Epoch 277/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.4905 - mse: 291.3766\n",
      "Epoch 277: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 295.2904 - mse: 291.1764 - val_loss: 443.9549 - val_mse: 439.8417\n",
      "Epoch 278/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.9839 - mse: 289.8678\n",
      "Epoch 278: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 56ms/step - loss: 293.8526 - mse: 289.7366 - val_loss: 456.1736 - val_mse: 452.0569\n",
      "Epoch 279/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 294.2528 - mse: 290.1363\n",
      "Epoch 279: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 294.2528 - mse: 290.1363 - val_loss: 450.0490 - val_mse: 445.9308\n",
      "Epoch 280/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 295.3508 - mse: 291.2354\n",
      "Epoch 280: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 295.2819 - mse: 291.1665 - val_loss: 453.7175 - val_mse: 449.6035\n",
      "Epoch 281/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.3331 - mse: 290.2191\n",
      "Epoch 281: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 58ms/step - loss: 294.3432 - mse: 290.2291 - val_loss: 438.3453 - val_mse: 434.2307\n",
      "Epoch 282/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.0338 - mse: 289.9178\n",
      "Epoch 282: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 294.0088 - mse: 289.8929 - val_loss: 447.6575 - val_mse: 443.5387\n",
      "Epoch 283/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.9337 - mse: 289.8147\n",
      "Epoch 283: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 293.9337 - mse: 289.8147 - val_loss: 446.2386 - val_mse: 442.1193\n",
      "Epoch 284/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.1175 - mse: 289.9987\n",
      "Epoch 284: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 294.2549 - mse: 290.1362 - val_loss: 441.2574 - val_mse: 437.1380\n",
      "Epoch 285/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.0584 - mse: 289.9394\n",
      "Epoch 285: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 294.1989 - mse: 290.0800 - val_loss: 455.9101 - val_mse: 451.7920\n",
      "Epoch 286/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.0613 - mse: 289.9426\n",
      "Epoch 286: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 294.2687 - mse: 290.1501 - val_loss: 449.7816 - val_mse: 445.6641\n",
      "Epoch 287/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 295.2367 - mse: 291.1224\n",
      "Epoch 287: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 295.2367 - mse: 291.1224 - val_loss: 440.4508 - val_mse: 436.3358\n",
      "Epoch 288/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.9587 - mse: 290.8452\n",
      "Epoch 288: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 295.0909 - mse: 290.9774 - val_loss: 459.4006 - val_mse: 455.2919\n",
      "Epoch 289/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 294.6692 - mse: 290.5616\n",
      "Epoch 289: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 294.6692 - mse: 290.5616 - val_loss: 448.9705 - val_mse: 444.8602\n",
      "Epoch 290/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.9283 - mse: 289.8186\n",
      "Epoch 290: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 293.9283 - mse: 289.8186 - val_loss: 435.7676 - val_mse: 431.6563\n",
      "Epoch 291/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.6718 - mse: 289.5617\n",
      "Epoch 291: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 293.7072 - mse: 289.5970 - val_loss: 453.4966 - val_mse: 449.3839\n",
      "Epoch 292/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.8401 - mse: 290.7271\n",
      "Epoch 292: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 294.7169 - mse: 290.6039 - val_loss: 455.8985 - val_mse: 451.7885\n",
      "Epoch 293/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.5393 - mse: 289.4261\n",
      "Epoch 293: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 293.5393 - mse: 289.4261 - val_loss: 458.7953 - val_mse: 454.6795\n",
      "Epoch 294/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.5240 - mse: 289.4100\n",
      "Epoch 294: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 293.4866 - mse: 289.3726 - val_loss: 454.2071 - val_mse: 450.0895\n",
      "Epoch 295/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.3947 - mse: 290.2780\n",
      "Epoch 295: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 55ms/step - loss: 294.3614 - mse: 290.2447 - val_loss: 455.0872 - val_mse: 450.9757\n",
      "Epoch 296/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.5621 - mse: 289.4502\n",
      "Epoch 296: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 53ms/step - loss: 293.5621 - mse: 289.4502 - val_loss: 451.3295 - val_mse: 447.2168\n",
      "Epoch 297/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.0954 - mse: 289.9827\n",
      "Epoch 297: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 4s 53ms/step - loss: 294.1809 - mse: 290.0682 - val_loss: 457.0292 - val_mse: 452.9180\n",
      "Epoch 298/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.6662 - mse: 289.5546\n",
      "Epoch 298: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 54ms/step - loss: 293.5863 - mse: 289.4748 - val_loss: 451.1847 - val_mse: 447.0722\n",
      "Epoch 299/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 294.1731 - mse: 290.0603\n",
      "Epoch 299: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 57ms/step - loss: 294.1731 - mse: 290.0603 - val_loss: 472.3076 - val_mse: 468.1979\n",
      "Epoch 300/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.3002 - mse: 289.1880\n",
      "Epoch 300: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 62ms/step - loss: 293.3002 - mse: 289.1880 - val_loss: 448.0691 - val_mse: 443.9536\n",
      "Epoch 301/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.3035 - mse: 289.1877\n",
      "Epoch 301: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 293.3430 - mse: 289.2272 - val_loss: 454.0298 - val_mse: 449.9129\n",
      "Epoch 302/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 293.6755 - mse: 289.5609\n",
      "Epoch 302: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 64ms/step - loss: 293.6755 - mse: 289.5609 - val_loss: 455.7317 - val_mse: 451.6171\n",
      "Epoch 303/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 294.0819 - mse: 289.9682\n",
      "Epoch 303: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 6s 65ms/step - loss: 293.9851 - mse: 289.8715 - val_loss: 475.3228 - val_mse: 471.2115\n",
      "Epoch 304/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 293.8885 - mse: 289.7776\n",
      "Epoch 304: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 5s 59ms/step - loss: 293.9800 - mse: 289.8691 - val_loss: 446.4536 - val_mse: 442.3434\n",
      "Epoch 305/500\n",
      "84/85 [============================>.] - ETA: 0s - loss: 292.9346 - mse: 288.8228\n",
      "Epoch 305: val_loss did not improve from 364.46783\n",
      "85/85 [==============================] - 6s 66ms/step - loss: 293.0479 - mse: 288.9361 - val_loss: 467.1927 - val_mse: 463.0785\n",
      "Epoch 306/500\n",
      " 6/85 [=>............................] - ETA: 3s - loss: 291.1809 - mse: 287.0661"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Colin\\Desktop\\Coding\\ProblemBasedDL\\MusicNeuralNetwork\\music.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Step 5: Train the Model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x_train, y_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(x_valid, y_valid),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m4096\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[model_checkpoint]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Colin/Desktop/Coding/ProblemBasedDL/MusicNeuralNetwork/music.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Colin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 5: Train the Model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    epochs=500,\n",
    "    batch_size=4096,\n",
    "    callbacks=[model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as ses\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2869/2869 [==============================] - 5s 2ms/step\n",
      "WARNING: output should have shape (91801, 12) not (91801, 1). Broadcasting output.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluation\n",
    "plot_history(history=history, output_path=output_path)\n",
    "visualize(model, endpoint='timbre', subset='valid', output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display learning curve\n",
    "img_path = os.path.join(output_path, 'learning_curve.png')\n",
    "img = Image.open(img_path)\n",
    "img.show()\n",
    "\n",
    "# Display visualization\n",
    "img_path = os.path.join(output_path, 'visualize_valid.png')\n",
    "img = Image.open(img_path)\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
